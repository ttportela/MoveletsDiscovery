{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos para Paper Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root     = './../'\n",
    "data     = os.path.join(root, 'new_data')\n",
    "res_path = os.path.join(root, 'results')\n",
    "prg_path = os.path.join(root, 'programs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from automatize.preprocessing import kfold_trainAndTestSplit\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets - ?\n",
    " - A.R.: http://archive.ics.uci.edu/ml/datasets/Activity+Recognition+system+based+on+Multisensor+data+fusion+%28AReM%29\n",
    " - Sign Language: http://archive.ics.uci.edu/ml/datasets/Activity+Recognition+system+based+on+Multisensor+data+fusion+%28AReM%29\n",
    " - Geo-Magnetic field and WLAN dataset for indoor localisation from wristband and smartphone Data Set (http://archive.ics.uci.edu/ml/datasets/Geo-Magnetic+field+and+WLAN+dataset+for+indoor+localisation+from+wristband+and+smartphone)\n",
    " - Activities of Daily Living (ADLs) Recognition Using Binary Sensors Data Set (http://archive.ics.uci.edu/ml/datasets/Activities+of+Daily+Living+%28ADLs%29+Recognition+Using+Binary+Sensors)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer\n",
    "# OUTRO Maior: http://archive.ics.uci.edu/ml/datasets/Activity+Recognition+system+based+on+Multisensor+data+fusion+%28AReM%29\n",
    "dir_path = os.path.join(data, 'Activity Recognition')\n",
    "file = '1.csv'\n",
    "\n",
    "cols = ['seq','x','y','z','label']\n",
    "\n",
    "df = pd.read_csv(os.path.join(dir_path, file), names=cols)\n",
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://archive.ics.uci.edu/ml/datasets/clickstream+data+for+online+shopping\n",
    "# Interessante pra classificar o padrão de compra por país\n",
    "dir_path = os.path.join(data, 'eshop')\n",
    "file = 'e-shop clothing 2008.csv'\n",
    "\n",
    "df = pd.read_csv(os.path.join(dir_path, file), sep=';')\n",
    "df.country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://archive.ics.uci.edu/ml/datasets/Molecular+Biology+%28Promoter+Gene+Sequences%29\n",
    "dir_path = os.path.join(data, 'protein')\n",
    "file = 'promoters.data'\n",
    "\n",
    "df = pd.read_csv(os.path.join(dir_path, file), sep=',', names=['label', 'name', 'sequence'])\n",
    "df['sequence'] = df['sequence'].replace(\"\\t\", '', regex=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://archive.ics.uci.edu/ml/datasets/Molecular+Biology+%28Splice-junction+Gene+Sequences%29\n",
    "dir_path = os.path.join(data, 'splice-junction-gene-sequences')\n",
    "file = 'splice.data'\n",
    "\n",
    "df = pd.read_csv(os.path.join(dir_path, file), sep=',', names=['label', 'name', 'sequence'])\n",
    "# df['sequence'] = df['sequence'].replace(\"\\t\", '', regex=True)\n",
    "# df = df[['label', 'sequence']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df[0]['sequence'].str)\n",
    "ds = pd.DataFrame()\n",
    "tid = 1\n",
    "for row in df.iterrows():\n",
    "    aux = pd.DataFrame()\n",
    "    aux['sequence'] = list(row[1]['sequence'].strip())\n",
    "    aux['name'] = row[1]['name'].strip()\n",
    "    aux['label'] = row[1]['label'].strip()\n",
    "    aux['tid'] = tid\n",
    "    tid = tid + 1\n",
    "    ds = pd.concat([ds, aux])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_csv(os.path.join(dir_path, 'promoters_data.csv'), index = False) \n",
    "train, test = kfold_trainAndTestSplit(dir_path, 5, ds, random_num=1, class_col='label', \\\n",
    "                                      fileprefix='', columns_order=['tid','sequence', 'name', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zenodo.org/record/3884398#.X0bNJtNKjiw\n",
    "dir_path = os.path.join(data, 'GECCO_Water')\n",
    "file = '1_gecco2018_water_quality.csv'\n",
    "\n",
    "cols = [\"id\",\"Time\",\"Tp\",\"Cl\",\"pH\",\"Redox\",\"Leit\",\"Trueb\",\"Cl_2\",\"Fm\",\"Fm_2\",\"EVENT\"]\n",
    "\n",
    "df = pd.read_csv(os.path.join(dir_path, file))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.covidactnow.org/export-covid-act-now-data-spreadsheet/\n",
    "dir_path = os.path.join(data, 'covid_act')\n",
    "\n",
    "cols = ['date', 'hospitalBedsRequired', 'hospitalBedCapacity', 'ICUBedsInUse',\n",
    "       'ICUBedCapacity', 'ventilatorsInUse', 'ventilatorCapacity',\n",
    "       'RtIndicator', 'RtIndicatorCI90', 'cumulativeDeaths',\n",
    "       'cumulativeInfected', 'currentInfected', 'currentSusceptible',\n",
    "       'currentExposed', 'countryName', 'stateName', 'countyName',\n",
    "       'intervention', 'fips', 'lat', 'long', 'lastUpdatedDate']\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "df = pd.read_csv(os.path.join(dir_path, 'states.NO_INTERVENTION.timeseries.csv'))\n",
    "df['class'] = 'NO_INTERVENTION'\n",
    "dataset = pd.concat([dataset, df])\n",
    "\n",
    "df = pd.read_csv(os.path.join(dir_path, 'states.WEAK_INTERVENTION.timeseries.csv'))\n",
    "df['class'] = 'WEAK_INTERVENTION'\n",
    "dataset = pd.concat([dataset, df])\n",
    "\n",
    "df = pd.read_csv(os.path.join(dir_path, 'states.STRONG_INTERVENTION.timeseries.csv'))\n",
    "df['class'] = 'STRONG_INTERVENTION'\n",
    "dataset = pd.concat([dataset, df])\n",
    "# df.hospitalBedsRequired.unique()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df = joinTrainAndTest(dir_path, cols, train_file=\"specific_train.csv\", test_file=\"specific_test.csv\", class_col = 'label')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from automatize.analysis import def_random_seed, results2df, MLP, printLatex, loadData, RN4All, kFoldResults\n",
    "# def_random_seed(random_num=1, seed_num=1)\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'run1/Promoters'\n",
    "results     = os.path.join(res_path, 'HpL-5fold_4T_60G')\n",
    "\n",
    "# RN4All(results, prefix)\n",
    "ApproachEnsemble(os.path.join(results, prefix, 'HpL-specific'), modelfolder='model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'run4/Foursquare NYC'\n",
    "results     = os.path.join(res_path, 'hiper-compare_v2')\n",
    "\n",
    "# RN4All(results, prefix)\n",
    "# ApproachEnsemble(os.path.join(results, prefix, 'HpL-generic'), save_results=False)\n",
    "ApproachEnsemble(os.path.join(data, 'promoters'), save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = results2df(results, prefix)\n",
    "df = kFoldResults(results, 'Promoters', 'HpL-specific')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApproachEnsemble(dir_path, dir_path2, save_results=True, modelfolder='model'):\n",
    "        \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import tensorflow\n",
    "        \n",
    "    X_train, y_train, X_test, y_test = loadData(dir_path)\n",
    "    keys, vocab_size, num_classes, max_length, X_train2, y_train2, X_test2, y_test2 = get_trajectories(dir_path2)  \n",
    "        \n",
    "    labels   = list(set(y_train))\n",
    "    y_train  = [labels.index(x) for x in y_train]\n",
    "    y_test   = [labels.index(x) for x in y_test]\n",
    "    y_train2 = [labels.index(x) for x in y_train2]\n",
    "#     y_test2  = [labels.index(x) for x in y_test2]\n",
    "    \n",
    "    print(\"Building Ensemble models\")\n",
    "    time = datetime.now()\n",
    "    \n",
    "    tensorflow.keras.backend.clear_session()\n",
    "    pred1 = model_movelets(X_train, y_train, X_test, y_test)\n",
    "    tensorflow.keras.backend.clear_session()\n",
    "    pred2 = model_marc(keys, vocab_size, num_classes, max_length, X_train2, y_train2, X_test2, y_test2)\n",
    "\n",
    "    y_pred1 = [np.argmax(f) for f in pred1]\n",
    "    y_pred2 = [np.argmax(f) for f in pred2]\n",
    "\n",
    "    final_pred = (pred1*0.5+pred2*0.5)\n",
    "    y_pred = [np.argmax(f) for f in final_pred]\n",
    "    \n",
    "    print('Models results:')\n",
    "    print(get_line(y_test, y_pred1))\n",
    "    print(get_line(y_test, y_pred2))\n",
    "    \n",
    "    print('Ensembled results:')\n",
    "    line=get_line(y_test, y_pred)\n",
    "    print(line)\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------\n",
    "    if (save_results) :\n",
    "        if not os.path.exists(os.path.join(dir_path, modelfolder)):\n",
    "            os.makedirs(os.path.join(dir_path, modelfolder))\n",
    "        report = classification_report(y_test, classifier.predict(X_test) )\n",
    "        classification_report_csv(report, os.path.join(dir_path, modelfolder, \"model_approachEnsemble_report.csv\"),\"Ensemble\") \n",
    "        pd.DataFrame(line).to_csv(os.path.join(dir_path, modelfolder, \"model_approachEnsemble_history.csv\")) \n",
    "    \n",
    "    # ----------------------------------------------------------------------------------\n",
    "    time = (datetime.now()-time).total_seconds() * 1000\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    print(\"Done. \" + str(time) + \" milliseconds\")\n",
    "    print(\"---------------------------------------------------------------------------------\")\n",
    "    return time\n",
    "\n",
    "def get_line(y_true, y_pred):\n",
    "    acc = accuracy(y_true, y_pred)\n",
    "    f1  = f1_macro(y_true, y_pred)\n",
    "    prec= precision_macro(y_true, y_pred)\n",
    "    rec = recall_macro(y_true, y_pred)\n",
    "    accTop5 = 0 #calculateAccTop5(classifier, X_test, y_test, 5)\n",
    "    line=[acc, f1, prec, rec, accTop5]\n",
    "    return line\n",
    "\n",
    "def precision_macro(y_true, y_pred):\n",
    "    from sklearn.metrics import precision_score\n",
    "    return precision_score(y_true, y_pred, average='macro')\n",
    "def recall_macro(y_true, y_pred):\n",
    "    from sklearn.metrics import recall_score\n",
    "    return recall_score(y_true, y_pred, average='macro')\n",
    "def f1_macro(y_true, y_pred):\n",
    "    from sklearn.metrics import f1_score\n",
    "    return f1_score(y_true, y_pred, average='macro')\n",
    "def accuracy(y_true, y_pred):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    return accuracy_score(y_true, y_pred, normalize=True)\n",
    "\n",
    "def model_rf(keys, vocab_size, num_classes, max_length, x_train, y_train, x_test, y_test):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    classifier = RandomForestClassifier()\n",
    "\n",
    "    nx, nsamples, ny = np.shape(x_train)\n",
    "    x_train = x_train.reshape((nsamples,nx*ny))\n",
    "    nx, nsamples, ny = np.shape(x_test)\n",
    "    x_test = x_test.reshape((nsamples,nx*ny))\n",
    "\n",
    "    print(\"[Data Model:] Building random forrest\")\n",
    "    classifier = RandomForestClassifier(n_estimators=500, n_jobs = -1, random_state = 1, criterion = 'gini', bootstrap=True)\n",
    "    classifier.fit(x_train, y_train)\n",
    "    print(\"[Data Model:] OK\")\n",
    "\n",
    "    return classifier.predict_proba(x_test)\n",
    "\n",
    "#     return classifier\n",
    "\n",
    "def model_movelets(X_train, y_train, X_test, y_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout\n",
    "    from keras.optimizers import Adam\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from automatize.analysis import loadData\n",
    "    from automatize.Methods import classification_report, classification_report_csv, calculateAccTop5, f1\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # Neural Network - Definitions:\n",
    "    par_dropout = 0.5\n",
    "    par_batch_size = 200\n",
    "    par_epochs = 80\n",
    "    par_lr = 0.00095\n",
    "\n",
    "    # Building the neural network-\n",
    "    print(\"[Movelets:] Building neural network\")\n",
    "    lst_par_epochs = [80,50,50,30,20]\n",
    "    lst_par_lr = [0.00095,0.00075,0.00055,0.00025,0.00015]\n",
    "\n",
    "    nattr = len(X_train[1,:])    \n",
    "\n",
    "    # Scaling y and transforming to keras format\n",
    "    from sklearn import preprocessing\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train = le.transform(y_train) \n",
    "    y_test = le.transform(y_test)\n",
    "    from keras.utils import to_categorical\n",
    "    y_train1 = to_categorical(y_train)\n",
    "    y_test1 = to_categorical(y_test)\n",
    "    nclasses = len(le.classes_)\n",
    "    from keras import regularizers\n",
    "\n",
    "    #Initializing Neural Network\n",
    "    classifier = Sequential()\n",
    "    # Adding the input layer and the first hidden layer\n",
    "    classifier.add(Dense(units = 100, kernel_initializer = 'uniform', kernel_regularizer= regularizers.l2(0.02), activation = 'relu', input_dim = (nattr)))\n",
    "    #classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout( par_dropout )) \n",
    "    # Adding the output layer       \n",
    "    classifier.add(Dense(units = nclasses, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "    # Compiling Neural Network\n",
    "    adam = Adam(lr=par_lr)\n",
    "    classifier.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy','top_k_categorical_accuracy',f1])\n",
    "    # Fitting our model \n",
    "    history = classifier.fit(X_train, y_train1, validation_data = (X_test, y_test1), batch_size = par_batch_size, epochs = par_epochs, verbose=0)\n",
    "\n",
    "    print(\"[Movelets:] OK\")\n",
    "\n",
    "    return classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_marc(keys, vocab_size, num_classes, max_length, x_train, y_train, x_test, y_test):\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Dense, LSTM, GRU, Dropout\n",
    "    from keras.initializers import he_uniform\n",
    "    from keras.regularizers import l1\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.layers import Input, Add, Average, Concatenate, Embedding\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    \n",
    "    EMBEDDER_SIZE = 100\n",
    "    MERGE_TYPE    = 'concatenate' # 'add', 'average'\n",
    "    RNN_CELL      = 'lstm' # 'gru'\n",
    "    \n",
    "    CLASS_DROPOUT = 0.5\n",
    "    CLASS_HIDDEN_UNITS = 100\n",
    "    CLASS_LRATE = 0.001\n",
    "    CLASS_BATCH_SIZE = 64\n",
    "    CLASS_EPOCHS = 1000\n",
    "    EARLY_STOPPING_PATIENCE = 30\n",
    "    BASELINE_METRIC = 'acc'\n",
    "    BASELINE_VALUE = 0.5\n",
    "    \n",
    "    inputs = []\n",
    "    embeddings = []\n",
    "\n",
    "    for idx, key in enumerate(keys):\n",
    "        if key == 'lat_lon':\n",
    "            i = Input(shape=(max_length, vocab_size[key]),\n",
    "                      name='input_' + key)\n",
    "            e = Dense(units=EMBEDDER_SIZE,\n",
    "                      kernel_initializer=he_uniform(seed=1),\n",
    "                      name='emb_' + key)(i)\n",
    "        else:\n",
    "            i = Input(shape=(max_length,),\n",
    "                      name='input_' + key)\n",
    "            e = Embedding(vocab_size[key],\n",
    "                          EMBEDDER_SIZE,\n",
    "                          input_length=max_length,\n",
    "                          name='emb_' + key)(i)\n",
    "        inputs.append(i)\n",
    "        embeddings.append(e)\n",
    "\n",
    "    if len(embeddings) == 1:\n",
    "        hidden_input = embeddings[0]\n",
    "    elif MERGE_TYPE == 'add':\n",
    "        hidden_input = Add()(embeddings)\n",
    "    elif MERGE_TYPE == 'average':\n",
    "        hidden_input = Average()(embeddings)\n",
    "    else:\n",
    "        hidden_input = Concatenate(axis=2)(embeddings)\n",
    "\n",
    "    hidden_dropout = Dropout(CLASS_DROPOUT)(hidden_input)\n",
    "\n",
    "    if RNN_CELL == 'lstm':\n",
    "        rnn_cell = LSTM(units=CLASS_HIDDEN_UNITS,\n",
    "                        recurrent_regularizer=l1(0.02))(hidden_dropout)\n",
    "    else:\n",
    "        rnn_cell = GRU(units=CLASS_HIDDEN_UNITS,\n",
    "                       recurrent_regularizer=l1(0.02))(hidden_dropout)\n",
    "\n",
    "    rnn_dropout = Dropout(CLASS_DROPOUT)(rnn_cell)\n",
    "\n",
    "    softmax = Dense(units=num_classes,\n",
    "                    kernel_initializer=he_uniform(),\n",
    "                    activation='softmax')(rnn_dropout)\n",
    "\n",
    "    classifier = Model(inputs=inputs, outputs=softmax)\n",
    "    opt = Adam(lr=CLASS_LRATE)\n",
    "\n",
    "    classifier.compile(optimizer=opt,\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['acc'])\n",
    "\n",
    "    classifier.fit(x=x_train,\n",
    "                   y=y_train,\n",
    "                   validation_data=(x_test, y_test),\n",
    "                   batch_size=CLASS_BATCH_SIZE,\n",
    "                   shuffle=True,\n",
    "                   epochs=CLASS_EPOCHS,\n",
    "                   verbose=0)\n",
    "    \n",
    "    return classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train and test data from... ./../results/HpL-5fold_4T_60G/run1/Promoters/HpL-specific\n",
      "Done.\n",
      "Number of attributes: 538\n",
      "Loading data from file(s) ./../new_data/promoters/run1... \n",
      "Attribute 'sequence': 4 unique values\n",
      "Attribute 'name': 106 unique values\n",
      "Total of attribute/value pairs: 110\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'file_str' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2207d961da64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HpL-5fold_4T_60G'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mApproachEnsemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HpL-specific'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'promoters'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-ab1dce352699>\u001b[0m in \u001b[0;36mApproachEnsemble\u001b[0;34m(dir_path, dir_path2, save_results, modelfolder)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlabels\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-dac45b724691>\u001b[0m in \u001b[0;36mget_trajectories\u001b[0;34m(dir_path, tid_col, label_col, geo_precision, drop)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading data from files \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_str\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"... DONE!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_str' is not defined"
     ]
    }
   ],
   "source": [
    "run = 'run1'\n",
    "prefix = run+'/Promoters'\n",
    "results     = os.path.join(res_path, 'HpL-5fold_4T_60G')\n",
    "\n",
    "ApproachEnsemble(os.path.join(results, prefix, 'HpL-specific'), os.path.join(data, 'promoters', run), save_results=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 'run4'\n",
    "prefix = run+'/Foursquare NYC'\n",
    "results     = os.path.join(res_path, 'hiper-compare_v2')\n",
    "\n",
    "ApproachEnsemble(os.path.join(results, prefix, 'HpL-specific'), os.path.join(root, 'data', 'foursquare_nyc', run), save_results=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectories(dir_path, tid_col='tid',\n",
    "                     label_col='label', geo_precision=8, drop=[]):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    from marc.core.utils.geohash import bin_geohash\n",
    "    print(\"Loading data from file(s) \" + dir_path + \"... :D\")\n",
    "    df_train = pd.read_csv(os.path.join(dir_path, 'train.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(dir_path, 'test.csv'))\n",
    "    df = df_train.copy().append(df_test)\n",
    "    tids_train = df_train[tid_col].unique()\n",
    "\n",
    "    keys = list(df.keys())\n",
    "    vocab_size = {}\n",
    "    keys.remove(tid_col)\n",
    "\n",
    "    for col in drop:\n",
    "        if col in keys:\n",
    "            keys.remove(col)\n",
    "            print(\"Column '\" + col + \"' dropped \" +\n",
    "                       \"from input file!\")\n",
    "        else:\n",
    "            print(\"Column '\" + col + \"' cannot be \" +\n",
    "                       \"dropped because it was not found!\")\n",
    "\n",
    "    num_classes = len(set(df[label_col]))\n",
    "    count_attr = 0\n",
    "    lat_lon = False\n",
    "\n",
    "    if 'lat' in keys and 'lon' in keys:\n",
    "        keys.remove('lat')\n",
    "        keys.remove('lon')\n",
    "        lat_lon = True\n",
    "        count_attr += geo_precision * 5\n",
    "        print(\"Attribute Lat/Lon: \" +\n",
    "                   str(geo_precision * 5) + \"-bits value\")\n",
    "\n",
    "    for attr in keys:\n",
    "\n",
    "        if attr != label_col:\n",
    "            df[attr] = LabelEncoder().fit_transform(df[attr])\n",
    "            vocab_size[attr] = max(df[attr]) + 1\n",
    "        \n",
    "            values = len(set(df[attr]))\n",
    "            count_attr += values\n",
    "            print(\"Attribute '\" + attr + \"': \" +\n",
    "                       str(values) + \" unique values\")\n",
    "\n",
    "    print(\"Total of attribute/value pairs: \" +\n",
    "               str(count_attr))\n",
    "    keys.remove(label_col)\n",
    "\n",
    "    x = [[] for key in keys]\n",
    "    y = []\n",
    "    idx_train = []\n",
    "    idx_test = []\n",
    "    max_length = 0\n",
    "    trajs = len(set(df[tid_col]))\n",
    "\n",
    "    if lat_lon:\n",
    "        x.append([])\n",
    "\n",
    "    for idx, tid in enumerate(set(df[tid_col])):\n",
    "        traj = df.loc[df[tid_col].isin([tid])]\n",
    "        features = np.transpose(traj.loc[:, keys].values)\n",
    "\n",
    "        for i in range(0, len(features)):\n",
    "            x[i].append(features[i])\n",
    "\n",
    "        if lat_lon:\n",
    "            loc_list = []\n",
    "            for i in range(0, len(traj)):\n",
    "                lat = traj['lat'].values[i]\n",
    "                lon = traj['lon'].values[i]\n",
    "                loc_list.append(bin_geohash(lat, lon, geo_precision))\n",
    "            x[-1].append(loc_list)\n",
    "\n",
    "        label = traj[label_col].iloc[0]\n",
    "        y.append(label)\n",
    "\n",
    "        if tid in tids_train:\n",
    "            idx_train.append(idx)\n",
    "        else:\n",
    "            idx_test.append(idx)\n",
    "\n",
    "        if traj.shape[0] > max_length:\n",
    "            max_length = traj.shape[0]\n",
    "\n",
    "    if lat_lon:\n",
    "        keys.append('lat_lon')\n",
    "        vocab_size['lat_lon'] = geo_precision * 5\n",
    "\n",
    "    print(\"Loading data from files \" + dir_path + \"... DONE!\")\n",
    "    \n",
    "    x_train = np.asarray([[x[j][i] for i in idx_train] for j in range(2)])\n",
    "    y_train = np.asarray([y[i] for i in idx_train])\n",
    "    x_test  = np.asarray([[x[j][i] for i in idx_test] for j in range(2)])\n",
    "    y_test  = np.asarray([y[i] for i in idx_test])\n",
    "\n",
    "    print('Trajectories:  ' + str(trajs))\n",
    "    print('Labels:        ' + str(len(keys)))\n",
    "    print('Train size:    ' + str(len(x_train[0]) / trajs))\n",
    "    print('Test size:     ' + str(len(x_test[0]) / trajs))\n",
    "    print('x_train shape: ' + str(x_train.shape))\n",
    "    print('y_train shape: ' + str(y_train.shape))\n",
    "    print('x_test shape:  ' + str(x_test.shape))\n",
    "    print('y_test shape:  ' + str(y_test.shape))\n",
    "    \n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    x_train = np.asarray([pad_sequences(f, max_length, padding='pre') for f in x_train])\n",
    "    x_test  = np.asarray([pad_sequences(f, max_length, padding='pre') for f in x_test])\n",
    "    \n",
    "\n",
    "    return (keys, vocab_size, num_classes, max_length,\n",
    "            x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2\n",
    "def get_trajectories(dir_path, tid_col='tid',\n",
    "                     label_col='label', geo_precision=8, drop=[]):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    from marc.core.utils.geohash import bin_geohash\n",
    "    print(\"Loading data from file(s) \" + dir_path + \"... \")\n",
    "    df_train = pd.read_csv(os.path.join(dir_path, 'train.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(dir_path, 'test.csv'))\n",
    "    df = df_train.copy().append(df_test)\n",
    "    tids_train = df_train[tid_col].unique()\n",
    "\n",
    "    keys = list(df.keys())\n",
    "    vocab_size = {}\n",
    "    keys.remove(tid_col)\n",
    "\n",
    "    for col in drop:\n",
    "        if col in keys:\n",
    "            keys.remove(col)\n",
    "            print(\"Column '\" + col + \"' dropped \" +\n",
    "                       \"from input file!\")\n",
    "        else:\n",
    "            print(\"Column '\" + col + \"' cannot be \" +\n",
    "                       \"dropped because it was not found!\")\n",
    "\n",
    "    num_classes = len(set(df[label_col]))\n",
    "    count_attr = 0\n",
    "    lat_lon = False\n",
    "\n",
    "    if 'lat' in keys and 'lon' in keys:\n",
    "        keys.remove('lat')\n",
    "        keys.remove('lon')\n",
    "        lat_lon = True\n",
    "        count_attr += geo_precision * 5\n",
    "        print(\"Attribute Lat/Lon: \" +\n",
    "                   str(geo_precision * 5) + \"-bits value\")\n",
    "\n",
    "    for attr in keys:\n",
    "        if attr != label_col:\n",
    "            df[attr] = LabelEncoder().fit_transform(df[attr])\n",
    "            vocab_size[attr] = max(df[attr]) + 1\n",
    "\n",
    "            values = len(set(df[attr]))\n",
    "            count_attr += values\n",
    "            print(\"Attribute '\" + attr + \"': \" +\n",
    "                       str(values) + \" unique values\")\n",
    "\n",
    "    print(\"Total of attribute/value pairs: \" +\n",
    "               str(count_attr))\n",
    "    keys.remove(label_col)\n",
    "\n",
    "    x = [[] for key in keys]\n",
    "    y = []\n",
    "    idx_train = []\n",
    "    idx_test = []\n",
    "    max_length = 0\n",
    "    trajs = len(set(df[tid_col]))\n",
    "\n",
    "    if lat_lon:\n",
    "        x.append([])\n",
    "\n",
    "    for idx, tid in enumerate(set(df[tid_col])):\n",
    "        traj = df.loc[df[tid_col].isin([tid])]\n",
    "        features = np.transpose(traj.loc[:, keys].values)\n",
    "\n",
    "        for i in range(0, len(features)):\n",
    "            x[i].append(features[i])\n",
    "\n",
    "        if lat_lon:\n",
    "            loc_list = []\n",
    "            for i in range(0, len(traj)):\n",
    "                lat = traj['lat'].values[i]\n",
    "                lon = traj['lon'].values[i]\n",
    "                loc_list.append(bin_geohash(lat, lon, geo_precision))\n",
    "            x[-1].append(loc_list)\n",
    "\n",
    "        label = traj[label_col].iloc[0]\n",
    "        y.append(label)\n",
    "\n",
    "        if tid in tids_train:\n",
    "            idx_train.append(idx)\n",
    "        else:\n",
    "            idx_test.append(idx)\n",
    "\n",
    "        if traj.shape[0] > max_length:\n",
    "            max_length = traj.shape[0]\n",
    "\n",
    "    if lat_lon:\n",
    "        keys.append('lat_lon')\n",
    "        vocab_size['lat_lon'] = geo_precision * 5\n",
    "\n",
    "    one_hot_y = OneHotEncoder().fit(df.loc[:, [label_col]])\n",
    "\n",
    "    x = [np.asarray(f) for f in x]\n",
    "    y = one_hot_y.transform(pd.DataFrame(y)).toarray()\n",
    "    print(\"Loading data from files ... DONE!\")\n",
    "    \n",
    "    x_train = np.asarray([f[idx_train] for f in x])\n",
    "    y_train = y[idx_train]\n",
    "    x_test = np.asarray([f[idx_test] for f in x])\n",
    "    y_test = y[idx_test]\n",
    "\n",
    "    print('Trajectories:  ' + str(trajs))\n",
    "    print('Labels:        ' + str(len(keys)))\n",
    "    print('Train size:    ' + str(len(x_train[0]) / trajs))\n",
    "    print('Test size:     ' + str(len(x_test[0]) / trajs))\n",
    "    print('x_train shape: ' + str(x_train.shape))\n",
    "    print('y_train shape: ' + str(y_train.shape))\n",
    "    print('x_test shape:  ' + str(x_test.shape))\n",
    "    print('y_test shape:  ' + str(y_test.shape))\n",
    "    \n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    x_train = np.asarray([pad_sequences(f, max_length, padding='pre') for f in x_train])\n",
    "    x_test  = np.asarray([pad_sequences(f, max_length, padding='pre') for f in x_test])\n",
    "\n",
    "    return (keys, vocab_size, num_classes, max_length,\n",
    "            x_train, x_test,\n",
    "            y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file(s) ./../new_data/promoters/run1... :D\n",
      "Attribute 'sequence': 4 unique values\n",
      "Attribute 'name': 106 unique values\n",
      "Total of attribute/value pairs: 110\n",
      "Loading data from files ./../new_data/promoters/run1... DONE!\n",
      "Trajectories:  106\n",
      "Labels:        2\n",
      "Train size:    0.7924528301886793\n",
      "Test size:     0.20754716981132076\n",
      "x_train shape: (2, 84, 57)\n",
      "y_train shape: (84,)\n",
      "x_test shape:  (2, 22, 57)\n",
      "y_test shape:  (22,)\n"
     ]
    }
   ],
   "source": [
    "keys, vocab_size, num_classes, max_length, \\\n",
    "X_train3, y_train3, X_test3, y_test3 = get_trajectories(os.path.join(data, 'promoters', 'run1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '+', '-', '-',\n",
       "       '-', '-', '-', '-', '-', '-', '-', '-', '-'], dtype='<U1')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e9ba6bdc4e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train3))\n",
    "print(np.shape(y_train3))\n",
    "print(np.shape(X_test3))\n",
    "print(np.shape(y_test3))\n",
    "(keys, vocab_size, num_classes, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data(keys, vocab_size, num_classes, max_length, x_train, y_train, x_test, y_test):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Dense, LSTM, GRU, Dropout\n",
    "    from keras.initializers import he_uniform\n",
    "    from keras.regularizers import l1\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.layers import Input, Add, Average, Concatenate, Embedding\n",
    "    from keras.callbacks import EarlyStopping\n",
    "\n",
    "    CLASS_DROPOUT = 0.5\n",
    "    CLASS_HIDDEN_UNITS = 100\n",
    "    CLASS_LRATE = 0.001\n",
    "    CLASS_BATCH_SIZE = 64\n",
    "    CLASS_EPOCHS = 1000\n",
    "    EARLY_STOPPING_PATIENCE = 30\n",
    "    BASELINE_METRIC = 'acc'\n",
    "    BASELINE_VALUE = 0.5\n",
    "    EMBEDDER_SIZE = 100\n",
    "    MERGE_TYPE = 'concatenate'\n",
    "    RNN_CELL = 'gru'\n",
    "\n",
    "#     num_classes = 1\n",
    "\n",
    "    inputs = []\n",
    "    embeddings = []\n",
    "    for idx, key in enumerate(keys):\n",
    "        if key == 'lat_lon':\n",
    "            i = Input(shape=(max_length, vocab_size[key]),\n",
    "                      name='input_' + key)\n",
    "            e = Dense(units=EMBEDDER_SIZE,\n",
    "                      kernel_initializer=he_uniform(seed=1),\n",
    "                      name='emb_' + key)(i)\n",
    "        else:\n",
    "            i = Input(shape=(max_length,),\n",
    "                      name='input_' + key)\n",
    "            e = Embedding(vocab_size[key],\n",
    "                          EMBEDDER_SIZE,\n",
    "                          input_length=max_length,\n",
    "                          name='emb_' + key)(i)\n",
    "        inputs.append(i)\n",
    "        embeddings.append(e)\n",
    "\n",
    "    if len(embeddings) == 1:\n",
    "        hidden_input = embeddings[0]\n",
    "    if MERGE_TYPE == 'add':\n",
    "        hidden_input = Add()(embeddings)\n",
    "    elif MERGE_TYPE == 'average':\n",
    "        hidden_input = Average()(embeddings)\n",
    "    else:       \n",
    "        hidden_input = Concatenate(axis=2)(embeddings)\n",
    "\n",
    "    hidden_dropout = Dropout(CLASS_DROPOUT)(hidden_input)\n",
    "\n",
    "    if RNN_CELL == 'lstm':\n",
    "        rnn_cell = LSTM(units=CLASS_HIDDEN_UNITS,\n",
    "                        recurrent_regularizer=l1(0.02))(hidden_dropout)\n",
    "    else:\n",
    "        rnn_cell = GRU(units=CLASS_HIDDEN_UNITS,\n",
    "                       recurrent_regularizer=l1(0.02))(hidden_dropout)\n",
    "\n",
    "    rnn_dropout = Dropout(CLASS_DROPOUT)(rnn_cell)\n",
    "    softmax = Dense(units=num_classes,\n",
    "                    kernel_initializer=he_uniform(),\n",
    "                    activation='softmax')(rnn_dropout)\n",
    "#                     activation='sigmoid')(rnn_dropout)\n",
    "\n",
    "    classifier = Model(inputs=inputs, outputs=softmax)\n",
    "    opt = Adam(lr=CLASS_LRATE)\n",
    "\n",
    "    classifier.compile(optimizer=opt,\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['acc'])\n",
    "\n",
    "#     classifier.fit(x=np.array(x_train),\n",
    "#                    y=np.array(y_train),\n",
    "    classifier.fit(x=x_train,\n",
    "                   y=y_train,\n",
    "                   validation_data=(x_test, y_test),\n",
    "#                    validation_data=(np.array(x_test), np.array(y_test)),\n",
    "                   batch_size=CLASS_BATCH_SIZE,\n",
    "                   shuffle=True,\n",
    "                   epochs=CLASS_EPOCHS,\n",
    "                   verbose=0)\n",
    "\n",
    "    return classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================== PARAMS ====================================\n",
      "TRAIN_FILE = ../new_data/promoters/run1/sequence_train.csv\n",
      "TEST_FILE = ../new_data/promoters/run1/sequence_test.csv\n",
      "METRICS_FILE = ../results/MARC-specific_results.csv\n",
      "DATASET = MARC-specific\n",
      "EMBEDDER_SIZE = 100\n",
      "MERGE_TYPE = concatenate \n",
      "\n",
      "[    INFO    ] 2020-09-04 19:18:41 :: Loading data from file(s) '../new_data/promoters/run1/sequence_train.csv' and '../new_data/promoters/run1/sequence_test.csv'... \n",
      "[    INFO    ] 2020-09-04 19:18:41 :: Attribute 'sequence': 4 unique values\n",
      "[    INFO    ] 2020-09-04 19:18:41 :: Total of attribute/value pairs: 4\n",
      "\u001b[K[    INFO    ] 2020-09-04 19:18:41 :: Processing trajectory 106/106. \n",
      "[    INFO    ] 2020-09-04 19:18:41 :: Loading data from files '../new_data/promoters/run1/sequence_train.csv' and '../new_data/promoters/run1/sequence_test.csv'... DONE!\n",
      "[    INFO    ] 2020-09-04 19:18:41 :: Trajectories:  106\n",
      "[    INFO    ] 2020-09-04 19:18:41 :: Labels:        2\n",
      "[    INFO    ] 2020-09-04 19:18:41 :: Train size:    0.7924528301886793\n",
      "[    INFO    ] 2020-09-04 19:18:41 :: Test size:     0.20754716981132076\n",
      "[    INFO    ] 2020-09-04 19:18:41 :: x_train shape: (1, 84, 57)\n",
      "[    INFO    ] 2020-09-04 19:18:41 :: y_train shape: (84, 2)\n",
      "[    INFO    ] 2020-09-04 19:18:41 :: x_test shape:  (1, 22, 57)\n",
      "[    INFO    ] 2020-09-04 19:18:41 :: y_test shape:  (22, 2)\n",
      "Using TensorFlow backend.\n",
      "===================================== OURS =====================================\n",
      "CLASS_DROPOUT = 0.5\n",
      "CLASS_HIDDEN_UNITS = 100\n",
      "CLASS_LRATE = 0.001\n",
      "CLASS_BATCH_SIZE = 64\n",
      "CLASS_EPOCHS = 1000\n",
      "EARLY_STOPPING_PATIENCE = 30\n",
      "BASELINE_METRIC = acc\n",
      "BASELINE_VALUE = 0.5 \n",
      "\n",
      "2020-09-04 19:19:02.478636: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb049379330 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-09-04 19:19:02.478678: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "===== Training Epoch 1 =====\n",
      "TRAIN\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.387500\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.399504\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 2 =====\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "TRAIN\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.333333\tprec_macro: 0.250000\trec_macro: 0.500000\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.333333\tprec_macro: 0.250000\trec_macro: 0.500000\n",
      "===== Training Epoch 3 =====\n",
      "TRAIN\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.333333\tprec_macro: 0.250000\trec_macro: 0.500000\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.333333\tprec_macro: 0.250000\trec_macro: 0.500000\n",
      "===== Training Epoch 4 =====\n",
      "TRAIN\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.333333\tprec_macro: 0.250000\trec_macro: 0.500000\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.333333\tprec_macro: 0.250000\trec_macro: 0.500000\n",
      "===== Training Epoch 5 =====\n",
      "TRAIN\t\tacc: 0.511905\tacc_top5: 1.000000\tf1_macro: 0.359256\tprec_macro: 0.753012\trec_macro: 0.511905\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.333333\tprec_macro: 0.250000\trec_macro: 0.500000\n",
      "===== Training Epoch 6 =====\n",
      "TRAIN\t\tacc: 0.547619\tacc_top5: 1.000000\tf1_macro: 0.431219\tprec_macro: 0.762500\trec_macro: 0.547619\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.333333\tprec_macro: 0.250000\trec_macro: 0.500000\n",
      "===== Training Epoch 7 =====\n",
      "TRAIN\t\tacc: 0.571429\tacc_top5: 1.000000\tf1_macro: 0.517857\tprec_macro: 0.628571\trec_macro: 0.571429\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.333333\tprec_macro: 0.250000\trec_macro: 0.500000\n",
      "===== Training Epoch 8 =====\n",
      "TRAIN\t\tacc: 0.547619\tacc_top5: 1.000000\tf1_macro: 0.507407\tprec_macro: 0.570707\trec_macro: 0.547619\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.399504\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 9 =====\n",
      "TRAIN\t\tacc: 0.559524\tacc_top5: 1.000000\tf1_macro: 0.530159\tprec_macro: 0.579365\trec_macro: 0.559524\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.399504\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 10 =====\n",
      "TRAIN\t\tacc: 0.559524\tacc_top5: 1.000000\tf1_macro: 0.523824\tprec_macro: 0.585020\trec_macro: 0.559524\n",
      "TEST\t\tacc: 0.545455\tacc_top5: 1.000000\tf1_macro: 0.427083\tprec_macro: 0.761905\trec_macro: 0.545455\n",
      "===== Training Epoch 11 =====\n",
      "TRAIN\t\tacc: 0.559524\tacc_top5: 1.000000\tf1_macro: 0.530159\tprec_macro: 0.579365\trec_macro: 0.559524\n",
      "TEST\t\tacc: 0.545455\tacc_top5: 1.000000\tf1_macro: 0.427083\tprec_macro: 0.761905\trec_macro: 0.545455\n",
      "===== Training Epoch 12 =====\n",
      "TRAIN\t\tacc: 0.571429\tacc_top5: 1.000000\tf1_macro: 0.559184\tprec_macro: 0.580357\trec_macro: 0.571429\n",
      "TEST\t\tacc: 0.454545\tacc_top5: 1.000000\tf1_macro: 0.371429\tprec_macro: 0.403509\trec_macro: 0.454545\n",
      "===== Training Epoch 13 =====\n",
      "TRAIN\t\tacc: 0.535714\tacc_top5: 1.000000\tf1_macro: 0.534063\tprec_macro: 0.536228\trec_macro: 0.535714\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.443678\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 14 =====\n",
      "TRAIN\t\tacc: 0.535714\tacc_top5: 1.000000\tf1_macro: 0.535648\tprec_macro: 0.535735\trec_macro: 0.535714\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.443678\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 15 =====\n",
      "TRAIN\t\tacc: 0.535714\tacc_top5: 1.000000\tf1_macro: 0.535648\tprec_macro: 0.535735\trec_macro: 0.535714\n",
      "TEST\t\tacc: 0.454545\tacc_top5: 1.000000\tf1_macro: 0.410714\tprec_macro: 0.435294\trec_macro: 0.454545\n",
      "===== Training Epoch 16 =====\n",
      "TRAIN\t\tacc: 0.547619\tacc_top5: 1.000000\tf1_macro: 0.547362\tprec_macro: 0.547727\trec_macro: 0.547619\n",
      "TEST\t\tacc: 0.454545\tacc_top5: 1.000000\tf1_macro: 0.410714\tprec_macro: 0.435294\trec_macro: 0.454545\n",
      "===== Training Epoch 17 =====\n",
      "TRAIN\t\tacc: 0.535714\tacc_top5: 1.000000\tf1_macro: 0.532468\tprec_macro: 0.536735\trec_macro: 0.535714\n",
      "TEST\t\tacc: 0.454545\tacc_top5: 1.000000\tf1_macro: 0.410714\tprec_macro: 0.435294\trec_macro: 0.454545\n",
      "===== Training Epoch 18 =====\n",
      "TRAIN\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.489583\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.498965\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 19 =====\n",
      "TRAIN\t\tacc: 0.523810\tacc_top5: 1.000000\tf1_macro: 0.505882\tprec_macro: 0.527851\trec_macro: 0.523810\n",
      "TEST\t\tacc: 0.454545\tacc_top5: 1.000000\tf1_macro: 0.454545\tprec_macro: 0.454545\trec_macro: 0.454545\n",
      "===== Training Epoch 20 =====\n",
      "TRAIN\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.475936\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "TEST\t\tacc: 0.454545\tacc_top5: 1.000000\tf1_macro: 0.454545\tprec_macro: 0.454545\trec_macro: 0.454545\n",
      "===== Training Epoch 21 =====\n",
      "TRAIN\t\tacc: 0.476190\tacc_top5: 1.000000\tf1_macro: 0.437614\tprec_macro: 0.467187\trec_macro: 0.476190\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.498965\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 22 =====\n",
      "TRAIN\t\tacc: 0.476190\tacc_top5: 1.000000\tf1_macro: 0.437614\tprec_macro: 0.467187\trec_macro: 0.476190\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.498965\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 23 =====\n",
      "TRAIN\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.475936\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "TEST\t\tacc: 0.454545\tacc_top5: 1.000000\tf1_macro: 0.454545\tprec_macro: 0.454545\trec_macro: 0.454545\n",
      "===== Training Epoch 24 =====\n",
      "TRAIN\t\tacc: 0.511905\tacc_top5: 1.000000\tf1_macro: 0.499927\tprec_macro: 0.513166\trec_macro: 0.511905\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.498965\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 25 =====\n",
      "TRAIN\t\tacc: 0.547619\tacc_top5: 1.000000\tf1_macro: 0.543478\tprec_macro: 0.549412\trec_macro: 0.547619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST\t\tacc: 0.454545\tacc_top5: 1.000000\tf1_macro: 0.450000\tprec_macro: 0.452991\trec_macro: 0.454545\n",
      "===== Training Epoch 26 =====\n",
      "TRAIN\t\tacc: 0.559524\tacc_top5: 1.000000\tf1_macro: 0.558961\tprec_macro: 0.559829\trec_macro: 0.559524\n",
      "TEST\t\tacc: 0.409091\tacc_top5: 1.000000\tf1_macro: 0.376906\tprec_macro: 0.385417\trec_macro: 0.409091\n",
      "===== Training Epoch 27 =====\n",
      "TRAIN\t\tacc: 0.535714\tacc_top5: 1.000000\tf1_macro: 0.535648\tprec_macro: 0.535735\trec_macro: 0.535714\n",
      "TEST\t\tacc: 0.454545\tacc_top5: 1.000000\tf1_macro: 0.410714\tprec_macro: 0.435294\trec_macro: 0.454545\n",
      "===== Training Epoch 28 =====\n",
      "TRAIN\t\tacc: 0.571429\tacc_top5: 1.000000\tf1_macro: 0.570455\tprec_macro: 0.572082\trec_macro: 0.571429\n",
      "TEST\t\tacc: 0.454545\tacc_top5: 1.000000\tf1_macro: 0.410714\tprec_macro: 0.435294\trec_macro: 0.454545\n",
      "===== Training Epoch 29 =====\n",
      "TRAIN\t\tacc: 0.595238\tacc_top5: 1.000000\tf1_macro: 0.586806\tprec_macro: 0.603704\trec_macro: 0.595238\n",
      "TEST\t\tacc: 0.545455\tacc_top5: 1.000000\tf1_macro: 0.476190\tprec_macro: 0.596491\trec_macro: 0.545455\n",
      "===== Training Epoch 30 =====\n",
      "TRAIN\t\tacc: 0.583333\tacc_top5: 1.000000\tf1_macro: 0.569609\tprec_macro: 0.595517\trec_macro: 0.583333\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.399504\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 31 =====\n",
      "TRAIN\t\tacc: 0.583333\tacc_top5: 1.000000\tf1_macro: 0.565539\tprec_macro: 0.599661\trec_macro: 0.583333\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.399504\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 32 =====\n",
      "TRAIN\t\tacc: 0.583333\tacc_top5: 1.000000\tf1_macro: 0.565539\tprec_macro: 0.599661\trec_macro: 0.583333\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.399504\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 33 =====\n",
      "TRAIN\t\tacc: 0.595238\tacc_top5: 1.000000\tf1_macro: 0.580000\tprec_macro: 0.611406\trec_macro: 0.595238\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.399504\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 34 =====\n",
      "TRAIN\t\tacc: 0.595238\tacc_top5: 1.000000\tf1_macro: 0.580000\tprec_macro: 0.611406\trec_macro: 0.595238\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.399504\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 35 =====\n",
      "TRAIN\t\tacc: 0.583333\tacc_top5: 1.000000\tf1_macro: 0.569609\tprec_macro: 0.595517\trec_macro: 0.583333\n",
      "TEST\t\tacc: 0.545455\tacc_top5: 1.000000\tf1_macro: 0.476190\tprec_macro: 0.596491\trec_macro: 0.545455\n",
      "===== Training Epoch 36 =====\n",
      "TRAIN\t\tacc: 0.583333\tacc_top5: 1.000000\tf1_macro: 0.578495\tprec_macro: 0.587344\trec_macro: 0.583333\n",
      "TEST\t\tacc: 0.500000\tacc_top5: 1.000000\tf1_macro: 0.443678\tprec_macro: 0.500000\trec_macro: 0.500000\n",
      "===== Training Epoch 37 =====\n",
      "TRAIN\t\tacc: 0.547619\tacc_top5: 1.000000\tf1_macro: 0.547362\tprec_macro: 0.547727\trec_macro: 0.547619\n",
      "TEST\t\tacc: 0.454545\tacc_top5: 1.000000\tf1_macro: 0.410714\tprec_macro: 0.435294\trec_macro: 0.454545\n",
      "===== Training Epoch 38 =====\n",
      "TRAIN\t\tacc: 0.571429\tacc_top5: 1.000000\tf1_macro: 0.571185\tprec_macro: 0.571591\trec_macro: 0.571429\n",
      "TEST\t\tacc: 0.409091\tacc_top5: 1.000000\tf1_macro: 0.376906\tprec_macro: 0.385417\trec_macro: 0.409091\n",
      "===== Training Epoch 39 =====\n",
      "TRAIN\t\tacc: 0.559524\tacc_top5: 1.000000\tf1_macro: 0.558961\tprec_macro: 0.559829\trec_macro: 0.559524\n",
      "TEST\t\tacc: 0.409091\tacc_top5: 1.000000\tf1_macro: 0.376906\tprec_macro: 0.385417\trec_macro: 0.409091\n",
      "===== Training Epoch 40 =====\n",
      "TRAIN\t\tacc: 0.535714\tacc_top5: 1.000000\tf1_macro: 0.534063\tprec_macro: 0.536228\trec_macro: 0.535714\n",
      "TEST\t\tacc: 0.409091\tacc_top5: 1.000000\tf1_macro: 0.397895\tprec_macro: 0.401786\trec_macro: 0.409091\n"
     ]
    }
   ],
   "source": [
    "!python \"marc/multi_feature_classifier.py\" \"../new_data/promoters/run1/sequence_train.csv\" \"../new_data/promoters/run1/sequence_test.csv\" \"../results/MARC-specific_results.csv\" \"MARC-specific\" 100 concatenate lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
